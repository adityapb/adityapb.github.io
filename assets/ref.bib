@article{pysph2021,
author = {Ramachandran, Prabhu and Bhosale, Aditya and Puri, Kunal and Negi, Pawan and Muta, Abhinav and Dinesh, A. and Menon, Dileep and Govind, Rahul and Sanka, Suraj and Sebastian, Amal S. and Sen, Ananyo and Kaushik, Rohan and Kumar, Anshuman and Kurapati, Vikas and Patil, Mrinalgouda and Tavker, Deep and Pandey, Pankaj and Kaushik, Chandrashekhar and Dutt, Arkopal and Agarwal, Arpit},
title = {PySPH: A Python-Based Framework for Smoothed Particle Hydrodynamics},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3460773},
doi = {10.1145/3460773},
abstract = {PySPH is an open-source, Python-based, framework for particle methods in general and Smoothed Particle Hydrodynamics (SPH) in particular. PySPH allows a user to define a complete SPH simulation using pure Python. High-performance code is generated from this high-level Python code and executed on either multiple cores, or on GPUs, seamlessly. It also supports distributed execution using MPI. PySPH supports a wide variety of SPH schemes and formulations. These include, incompressible and compressible fluid flow, elastic dynamics, rigid body dynamics, shallow water equations, and other problems. PySPH supports a variety of boundary conditions including mirror, periodic, solid wall, and inlet/outlet boundary conditions. The package is written to facilitate reuse and reproducibility. This article discusses the overall design of PySPH and demonstrates many of its features. Several example results are shown to demonstrate the range of features that PySPH provides.},
journal = {ACM Trans. Math. Softw.},
month = {sep},
articleno = {34},
numpages = {38},
keywords = {CPU, PySPH, GPU, smoothed particle hydrodynamics, open source, Python},
file = {pysph.pdf}
}

@inproceedings{ compyle2020,
  author    = { {A}ditya {B}hosale and {P}rabhu {R}amachandran },
  title     = { {C}ompyle: a {P}ython package for parallel computing },
  booktitle = { {P}roceedings of the 19th {P}ython in {S}cience {C}onference },
  pages     = { 32 - 39 },
  year      = { 2020 },
  editor    = { {M}eghann {A}garwal and {C}hris {C}alloway and {D}illon {N}iederhut and {D}avid {S}hupe },
  doi       = { 10.25080/Majora-342d178e-005 },
  file      = {compyle.pdf}
}

@inproceedings{ampi2023,
author = {Ramos, Evan and White, Sam and Bhosale, Aditya and Kale, Laxmikant},
title = {Runtime Techniques for Automatic Process Virtualization},
year = {2023},
isbn = {9781450394451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3547276.3548522},
doi = {10.1145/3547276.3548522},
abstract = {Asynchronous many-task runtimes look promising for the next generation of high performance computing systems. But these runtimes are usually based on new programming models, requiring extensive programmer effort to port existing applications to them. An alternative approach is to reimagine the execution model of widely used programming APIs, such as MPI, in order to execute them more asynchronously. Virtualization is a powerful technique that can be used to execute a bulk synchronous parallel program in an asynchronous manner. Moreover, if the virtualized entities can be migrated between address spaces, the runtime can optimize execution with dynamic load balancing, fault tolerance, and other adaptive techniques. Previous work on automating process virtualization has explored compiler approaches, source-to-source refactoring tools, and runtime methods. These approaches achieve virtualization with different tradeoffs in terms of portability (across different architectures, operating systems, compilers, and linkers), programmer effort required, and the ability to handle all different kinds of global state and programming languages. We implement support for three different related runtime methods, discuss shortcomings and their applicability to user-level virtualized process migration, and compare performance to existing approaches. Compared to existing approaches, one of our new methods achieves what we consider the best overall functionality in terms of portability, automation, support for migration, and runtime performance.},
booktitle = {Workshop Proceedings of the 51st International Conference on Parallel Processing},
articleno = {26},
numpages = {10},
location = {Bordeaux, France},
series = {ICPP Workshops '22}
}

@InProceedings{charmstencil2024,
author="Bhosale, Aditya
and Fink, Zane
and Kale, Laxmikant",
editor="Diehl, Patrick
and Schuchart, Joseph
and Valero-Lara, Pedro
and Bosilca, George",
title="An Abstraction forÂ Distributed Stencil Computations Using Charm++",
booktitle="Asynchronous Many-Task Systems and Applications",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="123--134",
abstract="Python has emerged as a popular programming language for scientific computing in recent years, thanks to libraries like Numpy and SciPy. Numpy, in particular, is widely utilized for prototyping numerical solvers using methods such as finite difference, finite volume, and multigrid. However, Numpy's performance is confined to a single node, compelling programmers to resort to a lower-level language for running large-scale simulations. In this paper, we introduce CharmStencil, a high-level abstraction featuring a Numpy-like Python frontend and a highly efficient Charm++ backend. Employing a client-server model, CharmStencil maintains productivity with tools like Jupyter notebooks on the frontend while utilizing a high-performance Charm++ library on the backend for computation. We demonstrate that CharmStencil achieves orders of magnitude better single-threaded performance compared to Numpy and can scale to thousands of CPU cores. Additionally, we showcase superior performance compared to cuNumeric and Numba, popular Python libraries for parallel array computations.",
isbn="978-3-031-61763-8"
}


@incollection{charmtyles2023,
    title        = {CharmTyles: Large-scale interactive Charm++ with Python},
    author       = {Aditya Bhosale and Nikunj Gupta and Zane Fink and Aryan Sharma},
    year         = 2023,
    booktitle    = {15th JLESC Workshop},
    address      = {INRIA, Bordeaux},
}

@incollection{kubernetes2023,
    title        = {Support for Charm++ on cloud using Kubernetes},
    author       = {Aditya Bhosale and Kavitha Chandrasekar and others},
    year         = 2023,
    booktitle    = {15th JLESC Workshop},
    address      = {INRIA, Bordeaux},
}

@incollection{charmtyles2022,
    title        = {CharmTyles: Large-scale interactive Charm++ with Python},
    author       = {Aditya Bhosale and Nikunj Gupta and Zane Fink and Aryan Sharma},
    year         = 2022,
    booktitle    = {20th Annual Workshop on Charm++ and its Applications},
    address      = {College Park, Maryland},
}

@incollection{kubernetes2022,
    title        = {Support for Charm++ on cloud using Kubernetes},
    author       = {Aditya Bhosale and Kavitha Chandrasekar and others},
    year         = 2022,
    booktitle    = {20th Annual Workshop on Charm++ and its Applications},
    address      = {College Park, Maryland},
}

@incollection{compyleposter2020,
    title        = {Compyle: Python Once, Parallel Computing Anywhere},
    author       = {Aditya Bhosale and Prabhu Ramachandran},
    year         = 2020,
    booktitle    = {SciPy Conference},
    address      = {Austin, Texas},
}

@incollection{pyopencl2017,
    title        = {GPU Computing using PyOpenCL},
    author       = {Aditya Bhosale},
    year         = 2017,
    booktitle    = {SciPy India Conference},
    address      = {Mumbai},
}

